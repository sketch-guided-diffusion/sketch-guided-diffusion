<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Sketch Guidance</title>
<link href="style.css" rel="stylesheet">
  <script type="text/javascript" src="./files/prompt_switch.js"></script>
</head>

<body>
<div class="content">
  <h1>Sketch-Guided Text-to-Image Diffusion Models</h1>
  <h2>SIGGRAPH 2023<h2>
  <p id="authors"><a href="https://scholar.google.com/citations?user=imBjSgUAAAAJ">Andrey Voynov<sup>1</sup></a> <a href="https://kfiraberman.github.io/">Kfir Aberman<sup>1</sup></a> <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or<sup>1,2</sup></a><br>

  <span style="font-size: 16px"><sup>1</sup> Google Research &nbsp;&nbsp;<sup>2</sup> Tel Aviv University
  </span></p>
  <br><img src="./files/teaser_bike.png" class="teaser-img" style="width:100%;"><br>

  <font size="+2">
    <p style="text-align: center;">
      <a href="./files/sketch-guided-preprint.pdf" target="_blank">Paper</a> &nbsp;&nbsp;&nbsp;&nbsp;
      <a>Code <span style="font-size: 14px;">(Coming Soon)</a>
    </p>
  </font>
</div>

<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Text-to-Image models have introduced a remarkable leap in the evolution of machine learning, demonstrating high-quality synthesis of images from a given text-prompt. However, these powerful pretrained models still lack control handles that can guide spatial properties of the synthesized images. 
    In this work, we introduce a universal approach to guide a pretrained text-to-image diffusion model, with a spatial map from another domain (e.g., sketch) during inference time. Unlike previous works, our method does not require  to train a dedicated model or a specialized encoder for the task.
    Our key idea is to train a Latent Guidance Predictor (LGP) - a small, per-pixel, Multi-Layer Perceptron (MLP) that maps latent features of noisy images to spatial maps, where the deep features are extracted from the core Denoising Diffusion Probabilistic Model (DDPM) network.
    The LGP is trained only on a few thousand of images and constitutes a differential guiding map predictor, over which the loss is computed and propagated back to push the intermediate images to agree with the spatial map.
    The per-pixel training offers flexibility and locality which allows the technique to perform well on out-of-domain sketches, including free-hand style drawings.
    We take a particular focus on the sketch-to-image translation task, revealing a robust and expressive way to generate images that follow the guidance of a sketch of arbitrary style or domain.
  </p>

  <br>

  <div>
    <img class="summary-img" src="./files/all_samples_promo_crop.jpg" style="width:100%;">
  </div> 
  <br>
</div>



<div class="content">
  <h2>Sketch-Guided Synthesis</h2>
  <p>Our method enables transfering hand-painted sketch into a natural highly-detailed image with a style control by any textual prompt.
  Here you can vary the generated image description while keeping alignement with the sketch.</p>

  <div class="prompt-sweep-main">
    <div class="prompt-sweep-text">
      <br>
      Sketch:
      <br>
      <div class="container"><img src="./files/prompt_switch/sketch.png", style="width:60%"></div>
      <br>
      Prompt:
      <div class="container">
      <br>
      <span class="prompt-sweep-bracket"><span class="prompt-sweep-button_a">Summer</span> <br> <span class="prompt-sweep-button_a">Dramatic</span> <br> <span class="prompt-sweep-button_a">Cold colors</span></span>
      &ensp;&ensp;&ensp;
      <span class="prompt-sweep-bracket"><span class="prompt-sweep-button_b">photograph of</span> <br> <span class="prompt-sweep-button_b">3d rendering of</span> <br> <span class="prompt-sweep-button_b">Marc Chagall painting of</span></span>
      &ensp;&ensp;&ensp;
      <span class="prompt-sweep-bracket"><span class="prompt-sweep-button_c">a stag</span> <br> <span class="prompt-sweep-button_c">a toy stag</span> <br> <span class="prompt-sweep-button_c">an origami stag</span> <br> <span class="prompt-sweep-button_c">a ghost stag</span></span>
      </div>
  </div> 
  <img class="prompt-sweep-img" id="prompt-sweep-img" src="./files/prompt_switch/default.png">
</div>

<br>
<p>Another sketch-to-image control is the fidelity to the edges. It can be varied by changing the spatial guidance stop time.</p>
<div class="prompt-sweep-main">
  <div class="prompt-sweep-text">
    <div class="container"><img src="./files/fidelity_seq/edges_ref.jpg", style="width:80%"></div>
    <span style="display: block; text-align: center">Edges fidelity </span><br>
    <div class="rotated">
    <input type="range" min="0" max="0.9" value="0.5" step="0.1" class="slider" id="range_fidelity">
    </div>
  </div>
  <br>
  <img class="prompt-sweep-img" id="fidelity-img" src="./files/fidelity_seq/seed-0_ps-8.0_es-1.6_gs-0.5.png"><br>
</div>

<br>
<p>This video shows the intermediate results for the sketch of a T-Rex drawing process.</p>
<div class="container">
<video width="400" height="400" controls>
  <source src="./files/output.webm", type="video/webm">
  Your browser does not support the video tag.
</video>
</div>
<br>

</div>



<div class="content">
<h2>Approach</h2>
  <p>
    The main idea is to perform a spatial guidance with gradients of a small model (we call it Latent Edge Predictor) that operates on intermediate DDPM activations.
  </p>

  <div>
    <p>
      Training scheme of the Latent Edge Predictor. Given an image, we first encode it and add noise to get a VQVAE representation.
      Then we pass it through the core U-net network of a DDPM, and extract a set of latent spatial features.
      Then our Latent Edge Predictor, which is a per-pixel MLP,
      is trained to map each pixel in the concatenated features to the corresponding pixel in the encoded edge map.
    </p>
    <img class="summary-img" src="files/scheme_train.jpg">
  </div>
  <div>
    <p>
      Sketch-Guided Text-to-Image Synthesis Scheme.
      Given an encoded noisy image, our method extracts its deep features during the inference process of a text-to-image diffusion model.
      In each of the denoising steps t, we aggregate the intermediate model features,
      and pass them in our per-pixel Latent Edge Predictor to predict the encoded edge map.
      Then we calculate the gradient of the similarity between the desired edges w.r.t. the input,
      and use it as a guidance for the denoising process that pushes the synthesized image to have edges close to the target edge map.
    </p>
    <img class="summary-img" src="files/scheme_inference.jpg">
  </div>
</div>



<div class="content">

  <h3>General Spatial Control</h3>
  <p>Our approach is applicable to any spatial attributes. This image demonstrates the spatial guidance being applied to an inpainting DDPM to suppress,
    and to increase saliency of a region on interest</p>
  <img class="summary-img" src="./files/saliency.jpg" style="width:90%;">

</div>

<div class="content">
  <h2>BibTex</h2>
  <code>  @article{voynov2022sketch,<br>
  &nbsp;&nbsp;title={Sketch-Guided Text-to-Image Diffusion Models},<br>
  &nbsp;&nbsp;author={Voynov, Andrey and Abernan, Kfir and Cohen-Or, Daniel},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arXiv:2211.13752},<br>
  &nbsp;&nbsp;year={2022}<br>
  } </code> 
</div>
<div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
We thank Chu Qinghao, Yael Vinker, Yael Pritch, Dani Valevski and David Salesin for their valuable inputs that helped improve this work. This page code is originated on <a href="https://dreambooth.github.io/" style="text-decoration: none; color: black;">DreamBooth</a>.
  </p>
</div>


</body>
</html>
